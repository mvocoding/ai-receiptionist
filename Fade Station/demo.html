<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fade Station · AI Demo</title>
    <meta name="description" content="Test AI Receptionist with Voice Chat">
    <link rel="icon"
        href="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 128 128'%3E%3Ccircle cx='64' cy='64' r='60' fill='%23000000'/%3E%3Cpath d='M40 80c8 10 40 10 48 0' stroke='%23ffffff' stroke-width='10' stroke-linecap='round' fill='none'/%3E%3Crect x='50' y='34' width='28' height='38' rx='14' fill='%23ffffff'/%3E%3C/svg%3E" />
    <script src="https://cdn.tailwindcss.com"></script>
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        sans: ["-apple-system", "BlinkMacSystemFont", "SF Pro Text", "Inter", "system-ui", "Segoe UI", "Roboto", "Helvetica Neue", "Arial", "Noto Sans"]
                    },
                    colors: {
                        ios: {
                            bg: "#0a0a0a",
                            card: "#111111",
                            card2: "#0d0d0d",
                            accent: "#0ea5e9",
                            accent2: "#22d3ee",
                            border: "#1f1f1f",
                            textMuted: "#9ca3af"
                        }
                    },
                    boxShadow: {
                        glow: "0 0 0 1px rgba(255,255,255,0.04), 0 10px 30px rgba(0,0,0,0.45)",
                    }
                }
            },
            darkMode: 'class'
        }
    </script>
</head>

<body class="min-h-screen bg-black text-white font-sans">
    <div class="mx-auto max-w-4xl px-4 py-6">
        <!-- Header -->
        <header class="sticky top-0 z-40">
            <div class="backdrop-blur-md bg-white/5 border border-ios-border rounded-2xl shadow-glow">
                <div class="flex items-center justify-between px-4 py-3">
                    <div class="flex items-center gap-3">
                        <a href="landing.html" class="flex items-center gap-3 hover:opacity-80 transition">
                            <div
                                class="h-8 w-8 rounded-xl bg-white/10 flex items-center justify-center backdrop-blur-md border border-white/10">
                                <span class="text-xs font-semibold">FS</span>
                            </div>
                            <div>
                                <h1 class="text-lg font-semibold tracking-tight">AI Demo</h1>
                                <p class="text-xs text-ios-textMuted">Voice Chat Testing</p>
                            </div>
                        </a>
                    </div>
                    <div class="flex items-center gap-2">
                        <a href="landing.html"
                            class="px-3 py-1.5 rounded-xl text-xs text-ios-textMuted hover:text-white transition">Home</a>
                        <a href="index.html"
                            class="px-3 py-1.5 rounded-xl text-xs text-ios-textMuted hover:text-white transition">Recordings</a>
                        <a href="communications.html"
                            class="px-3 py-1.5 rounded-xl text-xs text-ios-textMuted hover:text-white transition">Messages</a>
                        <a href="barbers.html"
                            class="px-3 py-1.5 rounded-xl text-xs text-ios-textMuted hover:text-white transition">Barbers</a>
                        <a href="training.html"
                            class="px-3 py-1.5 rounded-xl text-xs text-ios-textMuted hover:text-white transition">Training</a>
                        <a href="flow.html"
                            class="px-3 py-1.5 rounded-xl text-xs text-ios-textMuted hover:text-white transition">Flow</a>
                    </div>
                </div>
            </div>
        </header>

        <!-- Demo Section -->
        <div class="mt-6">
            <div
                class="bg-gradient-to-b from-ios-card to-ios-card2 border border-ios-border rounded-2xl shadow-glow p-6">
                <!-- Status -->
                <div class="flex items-center justify-between mb-6">
                    <div class="flex items-center gap-3">
                        <div class="relative">
                            <div id="statusIndicator" class="h-3 w-3 rounded-full bg-red-500"></div>
                            <div id="statusIndicatorPulse"
                                class="absolute inset-0 h-3 w-3 rounded-full bg-red-500 animate-ping opacity-75"></div>
                        </div>
                        <span id="statusText" class="text-sm text-ios-textMuted">Ready</span>
                    </div>
                    <button id="startStopBtn"
                        class="px-4 py-2 rounded-xl text-sm font-medium bg-emerald-500/90 hover:bg-emerald-500 transition shadow">
                        Start Chat
                    </button>
                </div>

                <!-- Controls -->
                <div class="flex items-center gap-4">
                    <div class="flex-1">
                        <div class="flex items-center gap-2">
                            <div class="flex-1 bg-white/5 border border-ios-border rounded-xl px-4 py-3">
                                <p id="transcriptionDisplay" class="text-sm text-ios-textMuted min-h-[24px]">Click
                                    "Start Chat" to begin voice conversation...</p>
                            </div>
                            <button id="clearBtn"
                                class="px-4 py-3 rounded-xl text-sm bg-white/10 border border-ios-border hover:bg-white/15 transition">
                                Clear
                            </button>
                        </div>
                    </div>
                </div>

            </div>
        </div>

        <footer class="py-8 text-center text-xs text-ios-textMuted">
            Fade Station · AI Demo Center
        </footer>
    </div>

    <script>
        const GEMINI_API_KEY = "AIzaSyAFU-AyH-DcxjRfTkU48M0rfcY3OpU6AoI";
        const GEMINI_WS_URL = `wss://generativelanguage.googleapis.com/ws/google.ai.generativelanguage.v1alpha.GenerativeService.BidiGenerateContent?key=${GEMINI_API_KEY}`;

        let ws = null;
        let audioContext = null;
        let microphone = null;
        let audioProcessor = null;
        let isRecording = false;
        let modelSpeaking = false;
        let audioQueue = [];
        let setupComplete = false;

        const statusIndicator = document.getElementById('statusIndicator');
        const statusIndicatorPulse = document.getElementById('statusIndicatorPulse');
        const statusText = document.getElementById('statusText');
        const startStopBtn = document.getElementById('startStopBtn');
        const clearBtn = document.getElementById('clearBtn');
        const transcriptionDisplay = document.getElementById('transcriptionDisplay');
        const whisperStatus = document.getElementById('whisperStatus');
        const geminiStatus = document.getElementById('geminiStatus');
        const lastError = document.getElementById('lastError');

        // Convert AudioBuffer to PCM16
        function audioBufferToPCM16(buffer) {
            const length = buffer.length;
            const pcm16 = new Int16Array(length);
            const samples = buffer.getChannelData(0);
            for (let i = 0; i < length; i++) {
                const s = Math.max(-1, Math.min(1, samples[i]));
                pcm16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
            }
            return pcm16;
        }

        // Convert PCM16 to base64
        function pcm16ToBase64(pcm16) {
            const bytes = new Uint8Array(pcm16.buffer);
            const binary = String.fromCharCode(...bytes);
            return btoa(binary);
        }

        // Play audio from queue
        async function playAudioQueue() {
            const AudioContextClass = window.AudioContext || window.webkitAudioContext;
            const audioCtx = new AudioContextClass({ sampleRate: 24000 });

            while (true) {
                if (audioQueue.length > 0) {
                    const audioData = audioQueue.shift();
                    const buffer = await audioCtx.decodeAudioData(audioData);
                    const source = audioCtx.createBufferSource();
                    source.buffer = buffer;
                    source.connect(audioCtx.destination);
                    source.start();
                    await new Promise(resolve => source.onended = resolve);
                } else {
                    await new Promise(resolve => setTimeout(resolve, 10));
                }
            }
        }

        async function startConnection() {
            try {
                ws = new WebSocket(GEMINI_WS_URL);

                ws.onopen = async () => {
                    geminiStatus.textContent = "Connected";
                    await ws.send(JSON.stringify({
                        setup: {
                            model: "models/gemini-2.0-flash-exp",
                            generation_config: {
                                system_instruction: {
                                    parts: [{
                                        text: "You are a helpful AI receptionist for Fade Station barbershop. Help with bookings, pricing ($45 fade, $40 taper, $35 line-up, $15 beard, $5 hot towel), barbers (Ace, Jay, Mia), and hours (Tue-Sat 9am-6pm). Be friendly and concise."
                                    }]
                                }
                            },
                            tools: []
                        }
                    }));
                    transcriptionDisplay.textContent = "Setting up...";
                };

                ws.onmessage = async (event) => {
                    let response;
                    try {
                        if (typeof event.data === 'string') {
                            response = JSON.parse(event.data);
                        } else if (event.data instanceof Blob) {
                            const text = await event.data.text();
                            response = JSON.parse(text);
                        } else {
                            // Unsupported frame type
                            return;
                        }
                    } catch (err) {
                        lastError.textContent = `Parse error: ${err?.message || err}`;
                        return;
                    }

                    // Handle setup complete
                    try {
                        if (response.setupComplete && !setupComplete) {
                            setupComplete = true;
                            transcriptionDisplay.textContent = "Connected! You can start talking now.";
                        }
                    } catch (e) { }

                    // Handle audio response
                    try {
                        const audioData = response.serverContent.modelTurn.parts[0].inlineData.data;
                        if (!modelSpeaking) {
                            modelSpeaking = true;
                            transcriptionDisplay.textContent = "AI is speaking...";
                        }
                        const binaryData = atob(audioData);
                        const bytes = new Uint8Array(binaryData.length);
                        for (let i = 0; i < binaryData.length; i++) {
                            bytes[i] = binaryData.charCodeAt(i);
                        }
                        audioQueue.push(bytes.buffer);
                    } catch (e) { }

                    // Handle turn complete
                    try {
                        const turnComplete = response.serverContent.turnComplete;
                        if (turnComplete) {
                            modelSpeaking = false;
                            transcriptionDisplay.textContent = "Ready to speak";
                        }
                    } catch (e) { }
                };

                ws.onerror = (error) => {
                    lastError.textContent = "WebSocket error: " + error;
                    geminiStatus.textContent = "Error";
                };

                ws.onclose = (event) => {
                    geminiStatus.textContent = "Disconnected";
                    transcriptionDisplay.textContent = "Connection closed";
                    lastError.textContent = `Close code: ${event.code}, reason: ${event.reason || 'none'}`;
                };

                // Start audio playback
                playAudioQueue();

            } catch (error) {
                lastError.textContent = error.message;
                geminiStatus.textContent = "Error";
            }
        }

        async function startRecording() {
            try {
                if (!ws || ws.readyState !== WebSocket.OPEN) {
                    await startConnection();
                }

                const stream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        channelCount: 1,
                        sampleRate: 16000,
                        echoCancellation: true,
                        noiseSuppression: true
                    }
                });

                audioContext = new AudioContext({ sampleRate: 16000 });
                microphone = audioContext.createMediaStreamSource(stream);

                audioProcessor = audioContext.createScriptProcessor(4096, 1, 1);

                audioProcessor.onaudioprocess = (event) => {
                    if (!modelSpeaking && ws && ws.readyState === WebSocket.OPEN) {
                        const inputData = event.inputBuffer.getChannelData(0);
                        const pcm16 = audioBufferToPCM16(event.inputBuffer);
                        const base64Audio = pcm16ToBase64(pcm16);

                        ws.send(JSON.stringify({
                            realtimeInput: {
                                mediaChunks: [{
                                    data: base64Audio,
                                    mimeType: "audio/pcm"
                                }]
                            }
                        }));
                    }
                };

                microphone.connect(audioProcessor);
                audioProcessor.connect(audioContext.destination);

                isRecording = true;
                statusIndicator.className = "h-3 w-3 rounded-full bg-emerald-500";
                statusIndicatorPulse.className = "absolute inset-0 h-3 w-3 rounded-full bg-emerald-500 animate-ping opacity-75";
                statusText.textContent = "Listening...";
                startStopBtn.textContent = "Stop Chat";
                startStopBtn.className = "px-4 py-2 rounded-xl text-sm font-medium bg-rose-500/90 hover:bg-rose-500 transition shadow";
                whisperStatus.textContent = "Streaming";

            } catch (error) {
                lastError.textContent = error.message;
                alert("Error accessing microphone: " + error.message);
            }
        }

        function stopRecording() {
            if (audioProcessor) {
                audioProcessor.disconnect();
            }
            if (microphone) {
                microphone.disconnect();
            }
            if (audioContext) {
                audioContext.close();
            }

            isRecording = false;
            modelSpeaking = false;

            statusIndicator.className = "h-3 w-3 rounded-full bg-red-500";
            statusIndicatorPulse.className = "absolute inset-0 h-3 w-3 rounded-full bg-red-500 animate-ping opacity-75";
            statusText.textContent = "Ready";
            startStopBtn.textContent = "Start Chat";
            startStopBtn.className = "px-4 py-2 rounded-xl text-sm font-medium bg-emerald-500/90 hover:bg-emerald-500 transition shadow";
            whisperStatus.textContent = "Ready";
            transcriptionDisplay.textContent = "Click Start Chat to begin";
        }

        startStopBtn.addEventListener('click', () => {
            if (isRecording) {
                stopRecording();
            } else {
                startRecording();
            }
        });

        clearBtn.addEventListener('click', () => {
            transcriptionDisplay.textContent = "Click Start Chat to begin";
        });

        // Initialize
        transcriptionDisplay.textContent = "Click 'Start Chat' to connect";
    </script>
</body>

</html>